from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI, OpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_core.documents import Document
from langchain_community.graphs.networkx_graph import NetworkxEntityGraph
from langchain.chains import GraphQAChain
from langchain.text_splitter import CharacterTextSplitter
from dotenv import load_dotenv
import os

CHROMA_PATH = "chroma"

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}"""

load_dotenv(override=True)

def load_llm(query_text, GRAPHRAG=True):
    """
    Load and execute the local LLM pipeline with the given query text.

    Args:
        query_text (str): The query text to process.
        GRAPHRAG (bool): If True, use GraphRAG for processing. If False, skip GraphRAG.

    Returns:
        str: The response text generated by the LLM.
    """
    # Retrieve from Vector Database
    embedding_function = OpenAIEmbeddings()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    results = db.similarity_search_with_relevance_scores(query_text, k=3)
    if len(results) == 0 or results[0][1] < 0.7:
        return "Unable to find matching results."

    # Combine Original Contexts
    original_context = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
    combined_context = original_context

    if GRAPHRAG:
        # Process with GraphRAG
        documents = [Document(page_content=doc.page_content) for doc, _score in results]
        llm = OpenAI()
        llm_transformer = LLMGraphTransformer(llm=llm)
        text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20, separator='\n')
        texts = text_splitter.split_documents(documents)
        graph_documents = llm_transformer.convert_to_graph_documents(texts)

        graph = NetworkxEntityGraph()

        for doc in graph_documents:
            for node in doc.nodes:
                graph.add_node(node.id)
            for edge in doc.relationships:
                graph._graph.add_edge(
                    edge.source.id,
                    edge.target.id,
                    relation=edge.type,
                )

        chain = GraphQAChain.from_llm(llm=llm, graph=graph, verbose=True)
        refined_context = chain.invoke({"query": query_text})

        # Combine refined context with original contexts
        combined_context = f"{refined_context}\n\n---\n\n{original_context}"

    # Generate Answer
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=combined_context, question=query_text)

    model = ChatOpenAI()
    response_text = model.invoke(prompt).content

    # Prepare Final Response
    sources = [doc.metadata.get("source", None) for doc, _score in results]
    formatted_response = f"Response: {response_text}\n\nSources: {sources}"
    return formatted_response


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("query_text", type=str, help="The query text.")
    parser.add_argument("--use_graphrag", action="store_true", help="Use GraphRAG for processing if set.")
    args = parser.parse_args()

    result = load_llm(args.query_text, GRAPHRAG=args.use_graphrag)
    print(result)

